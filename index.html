<html>
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <!-- CSS only -->
  <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.1/css/bootstrap.min.css" integrity="sha384-VCmXjywReHh4PwowAiWNagnWcLhlEJLA5buUprzK8rxFgeH0kww/aWY76TfkUoSX" crossorigin="anonymous">

  <!-- JS, Popper.js, and jQuery -->
  <script src="https://code.jquery.com/jquery-3.5.1.slim.min.js" integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.1/dist/umd/popper.min.js" integrity="sha384-9/reFTGAW83EW2RDu2S0VKaIzap3H66lZH81PoYlFhbGU+6BZp6G7niu735Sk7lN" crossorigin="anonymous"></script>
  <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.1/js/bootstrap.min.js" integrity="sha384-XEerZL0cuoUbHE4nZReLT7nx9gQrQreJekYhJD9WNWhH8nEW+0c5qq7aIo2Wl30J" crossorigin="anonymous"></script>

  <link href="assets/css/index.css" rel="stylesheet">

  <meta property="og:title" content="We Need Data to Improve Healthcare Processes at Scale">
  <meta property="og:image" content="assets/img/pancakes.png">
</head>
<body>
  <nav class="pt-2 my-md-3 border-bottom">
    <div class="row">
      <div class="col">
        <p class="text-center">Luis Erick Zul Rabasa</p>
      </div>
    </div>
  </nav>
  <div class="container">
    <div class="row">
      <div class='col'></div>
      <div class='col-10'>
      <h1>We Need Data to Improve Healthcare Processes at Scale</h1>
      <p class="text-justify text-break">
      <i>Note: the fictitious examples and diagrams are for illustrative purposes ONLY.
      They are mainly simplifications of real phenomena.
      Please consult with your physician if you have any questions.</i>
      <p class="text-justify text-break">

      <p class="text-justify text-break">
      <b>Scale is one of the main challenges in public health services</b>. Specialized treatments 
      are hard to track when applied to thousands of patients. Fortunately, we can now identify 
      bottlenecks and errors in the flow of these processes at scale. Through the combination 
      of Process Modelling and Probabilistic Modelling, precision medicine and stratified 
      healthcare has achieved state-of-the-art results like parallel process consolidation, 
      task connections, and task pruning.
      </p>

      <p class="text-justify text-break">Process modeling helps structure a series of steps to perform a task.</p>

      <img class="img-fluid" src="assets/img/1.jpg"></img>

      <p class="text-justify text-break">
      In the real world, tasks can run in parallel or series. Processes can have loops, 
      divergent and convergent paths, and unnecessary steps.
      </p>

      <img class="img-fluid" src="assets/img/2.jpg"></img>

      <p class="text-justify text-break">
      <b>Process modeling discovers such complexities and improves upon them. </b>
      Improving our process models is important since the main objective of stratified healthcare 
      and precision medicine is to achieve an individual, precise and complete treatment 
      for each patient. Stratified healthcare refers to the practice of offering a robust 
      infrastructure to treat patients, while precision medicine seeks to improve the health of 
      individual patients through personalized treatments.
      </p>

      <p class="text-justify text-break">
      Personalized treatments are a reality, thanks to the Human Genome Project. 
      The Human Genome Project demonstrates that no human is made the same. Therefore, 
      prescriptions and treatments cause different reactions in each individual. 
      Modeling processes allow us to reflect this reality in paper. 
      However, this method is not perfect.
      </p>

      <p class="text-justify text-break">
      While processes in health go through rigorous statistical tests, 
      they are by no means rid of bias. This bias can impact the flow of steps 
      or parallelism that could be applied to optimize positive outcomes or the speed of 
      the treatments. Additionally, the tools and tests in operations and treatments 
      evolve with time, which shortens steps or reduces the time overhead in these tasks. 
      Updating each process model every time a breakthrough is validated by experts 
      consumes a lot of time.
      </p>

      <p class="text-justify text-break">
      For example, imagine we currently have these personalized brain imaging 
      processes for 5 patients with a brain tumor:
      </p>

      <img class="img-fluid" src="assets/img/3.jpg"></img>

      <p class="text-justify text-break">
      Let's say we have a new technology called BrainImagingx2, which scans 
      a patient's tumor size two times faster than the current brain imaging technology, 
      but it's only precise for patients that can handle both MRI and CT scans periodically:
      </p>

      <img class="img-fluid" src="assets/img/4.jpg"></img>

      <p class="text-justify text-break">
      Naturally, we want to update our 5 model processes. This might be easy to do since 
      we're dealing with only 5 patients at this time. We can review each case manually 
      to evaluate the risk of updating the process model and weigh it against the benefit:
      </p>

      <img class="img-fluid" src="assets/img/5.jpg"></img>

      <p class="text-justify text-break">
      But what happens if suddenly our 5 patients turn into 1,000 different and 
      unique patients, each with their medical history and background? The cost of 
      evaluating each process model manually skyrockets. Even worse, maybe one of our 1,000 
      patients has a great risk of damage if exposed to the new technology in BrainImagingx2, 
      like electromagnetic fields. Under these conditions, we can't generalize or skip 
      updating models for time and effort optimization. In summary, process modeling's 
      key issue is time and effort from analyzing existing models thoroughly and updating models 
      with time. This gets worse when the number of patient profiles, and therefore the 
      processes modeled increase with scale.
      </p>

      <p class="text-justify text-break">
      <b>Process mining achieves an automated "understanding" of the processes through 
      the analysis of event logs and expert reviews to approximate processes. </b>
      Rather than attempting to update existing models, it builds models from the data it 
      is fed. The process of generating updated process models is thus faster. However, 
      it's not a perfect approach. Process mining outputs a greater number of personalized 
      healthcare models. However, generalization or aggregation can still happen since 
      most process mining techniques rely on process analysis, which automatically 
      filters, sorts and compresses the logs fed to the process. We need to avoid generalizing 
      the aggregation of our information to not obscure the edge case illnesses and 
      response our patients could have as a result of their unique genetic composition 
      like we discussed as a fact through the breakthrough of the Human Genome Project.
      </p>

      <p class="text-justify text-break">
      Another limitation to take into account is that this is an automated method 
      that relies on data, and the quality of the process models exported is directly related 
      to the quality of the information it is fed. The combined data of patients around 
      the world have variance and noise. The different scales (the what), and sometimes even 
      standards of the measurements (the how) impact what we see. Additionally, 
      Electronic Health Records are not always available across the world, so inconsistencies 
      between global records are highly likely. Missing data greatly degrades the quality 
      of these models. Language disparity obscures the understanding learned through these 
      data mining-based approaches.
      </p>

      <img class="img-fluid" src="assets/img/6.jpg"></img>
      Source: https://www.apadivisions.org/division-31/publications/records/intake

      <p class="text-justify text-break">
      We can combine Process Modelling, namely Process Mining, with Probabilistic Modelling 
      to better mitigate these issues. Probabilistic modeling takes uncertainty into account, 
      treating it as noise. A probabilistic model gives a distribution of possible outcomes. 
      By modeling outcomes as likelihoods, the uncertainty is quantified to be further 
      addressed and corrected by statistical methods.
      </p>

      <img class="img-fluid" src="assets/img/7.jpg"></img>

      <p class="text-justify text-break">
      Probabilistic Modelling assumes a relationship between the outcome and the independent 
      variable, however, this is something that needs to be proven. Correlation coefficients 
      and regression analysis are used to perform hypothesis tests. While correlation measures 
      the degree of association between two variables, other methods like regression measures 
      the predictive power of the independent variable should the relationship be represented 
      by a mathematical model. Probabilistic methods assume a model to account for noise and 
      to make inferences based on these models.
      </p>

      <p class="text-justify text-break">
      To combine both Process Modelling and Probabilistic Modelling, we can represent similar 
      healthcare processes through a single diagram, which is the result of grouping similar 
      patient task histories. In this diagram, each state transitions to multiple states. Each 
      transition has a probability based on the data that was grouped. This way, even if the models 
      are inconsistent with reality, they can be reviewed easily.
      </p>

      <img class="img-fluid" src="assets/img/8.jpg"></img>

      <p class="text-justify text-break">
      Additionally, statistical methods probability distribution fitting can be used to find 
      the distribution that best fits the process modeled. Some examples of these distributions 
      include the Poisson distribution for discrete variables like the task of vaccinating a patient 
      and the Exponential and Gamma distributions for continuous variables like the process of 
      assessing the levels of cholesterol in the body of a patient. Systems with queues, like those 
      of a coffee shop, a computer server, and more specifically a health clinic can be modeled as 
      probabilities through the theory of Queuing Systems, which incorporate the aforementioned 
      probability distributions. By fitting a distribution, you can determine to which degree the outcomes 
      in your processes are due to chance or not, which helps trim down or update the tasks in 
      processes that matter in healthcare.
      </p>

      <p class="text-justify text-break">
      Some of the latest applications of Probability Modelling in Process Modelling include 
      dealing with invisible prime tasks through rules and equations utilizing the probability 
      of state transition of Coupled Hidden Markov and double time-stamped in event logs. Invisible 
      prime tasks are usually run in parallel, and it's hard to map this parallelism from raw logs 
      back to XOR or AND gates in the diagrams without manual reviews. Through Coupled Hidden Markov Models, 
      "wherein all of states from each Hidden Markov Model are dependent on the states of all 
      Hidden Markov Model in previous time slice", mapping logs back to process diagrams can now 
      include such parallel tasks. (Sarno & Sungkono, 2016)
      </p>

      <img class="img-fluid" src="assets/img/9.jpg"></img>

      <p class="text-justify text-break">
      New learning methods for process mining based on probability modeling include using a logistic 
      regression model for the discovery of direct connections between events in event logs 
      with noise. (Maruster et al., 2002) Logistic regression addresses noise by treating the occurrence 
      of events as a binary outcome calculated with the independent variable mapped to the sigmoid 
      function, that for extreme positive values still gives a 1 and for extreme negative values 
      still gives a 0. Even so, this function exports a probability as a continuous variable which 
      allows us to show experts the probability of connecting events A and B, allowing further 
      assessment if needed.
      </p>

      <p class="text-justify text-break">
      There's a classic machine learning example that illustrates the limitations of using probabilistic 
      methods in process mining: you can't say a cancer test classifier is good enough if it predicts 
      that someone doesn't have cancer 90% of the time. This applies to models learned through 
      Probabilistic Modelling in Process Mining. If you have event logs that indicate that 90% of the 
      time after a test you should give a negative result for cancer, what does this probability mean? 
      Let's assume for a second that we didn't know what cancer was and decided that under an arbitrary 
      threshold, and that we were looking to prune out steps in our process to invest more money in those 
      with higher probability. What would happen if we decided to invest more money in the pathway of 
      the process for negative cancer cases, instead of positive cancer cases?
      <b>Interpreting the probabilities and steps requires a good understanding of their evaluation and 
      implementation. </b>
      To perform this automatically is a challenge that still needs to be addressed for clinical processes.
      </p>

      <img class="img-fluid" src="assets/img/10.jpg"></img>

      <p class="text-justify text-break">
      <b>Does correlation imply prediction?</b> The relationships found between the components of a process 
      diagram may not be true, they may as well be false negatives. For example, if you find two consecutive 
      event logs for a patient where one is the patient taking medication and the next one is the sickness 
      getting worse, just looking at these isolated logs could draw the naive conclusion that the medication 
      is to blame. However, this may not be the case. If more event logs, both in the past and the future 
      were taken into account, effects of. That's why correlation and regression alone are not enough to s
      olve these cases. A potential solution for this is the use of probability chains such as Markov Chains 
      and their extensions, that allow the probability of an event to be traced back to an earlier event 
      than its immediate predecessor.
      </p>

      <p class="text-justify text-break">
      <b>Health information is highly sensible.</b> It is a snapshot of the user's medical, pathological, 
      nutritional, and psychological history. In a world surrounded by taboos for mental illness and HIV, 
      this information needs to be handled with utmost care. Patients are served so that their information 
      is solely used for their recovery and rarely to be sold to third parties in a game of profit.
      </p>

      <img class="img-fluid" src="assets/img/11.jpg"></img>

      <p class="text-justify text-break">
      Process Modelling and Probabilistic Modelling methods aggregate from individual cases to 
      generalize and personalize during treatments when needed. <b> No matter how promising the results look, 
      we need to understand the potential edge cases and caveats of the methods used. </b>For example, the 
      classic photo classification example classified pictures of people of color as monkeys because of 
      the underrepresentation of their skin color in the data gathered for its training. In health, this 
      can lead to a misunderstanding from our models for things like HIV, where most data collected have 
      been documented for the queer community, especially gay men.
      </p>

      <h2>References</h2>
      <ul>
        <li>
          <p class="text-justify text-break">https://www.researchgate.net/profile/Bouchra_Marzak/publication/306046482_Clustering_in_Vehicular_Ad-Hoc_Network_Using_Artificial_Neural_Network/links/5a0c19f0a6fdccc69edaa37c/Clustering-in-Vehicular-Ad-Hoc-Network-Using-Artificial-Neural-Network.pdf#page=75</p>
        </li>
        <li>
          <p class="text-justify text-break">https://link.springer.com/chapter/10.1007/3-540-36182-0_37</p>
        </li>
        <li>
          <p class="text-justify text-break">https://neuroneurotic.net/2015/11/30/does-correlation-imply-prediction/</p>
        </li>
      </ul>
    </div>
    <div class='col'></div>
  </div>
  <footer class="pt-3 my-md-3 pt-md-3 border-top">
    <div class="row">
      <div class="col">
        <p class="text-center">Luis Erick Zul Rabasa © 2020 All Rights Reserved</p>
      </div>
    </div>
  </footer>
</body>
</html>